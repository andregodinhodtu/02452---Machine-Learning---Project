{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8242ed63",
   "metadata": {},
   "source": [
    "# reg_part_B_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04ba25",
   "metadata": {},
   "source": [
    "# Data import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13ac8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/lf/r13z9z5s0_1g8ylrsg2zq6k40000gn/T/ipykernel_82633/2274352155.py\", line 9, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4a57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data_file=\"heart_failure_clinical_records_dataset\"\n",
    "\n",
    "data = pd.read_csv(f\"../../raw_data/{name_data_file}.csv\", na_values=[\"?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac829f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>155000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2060</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>742000.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>395000.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0    75.0        0                       582         0                 20   \n",
       "1    55.0        0                      7861         0                 38   \n",
       "2    65.0        0                       146         0                 20   \n",
       "3    50.0        1                       111         0                 20   \n",
       "4    65.0        1                       160         1                 20   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "294  62.0        0                        61         1                 38   \n",
       "295  55.0        0                      1820         0                 38   \n",
       "296  45.0        0                      2060         1                 60   \n",
       "297  45.0        0                      2413         0                 38   \n",
       "298  50.0        0                       196         0                 45   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                      1  265000.00               1.9           130    1   \n",
       "1                      0  263358.03               1.1           136    1   \n",
       "2                      0  162000.00               1.3           129    1   \n",
       "3                      0  210000.00               1.9           137    1   \n",
       "4                      0  327000.00               2.7           116    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "294                    1  155000.00               1.1           143    1   \n",
       "295                    0  270000.00               1.2           139    0   \n",
       "296                    0  742000.00               0.8           138    0   \n",
       "297                    0  140000.00               1.4           140    1   \n",
       "298                    0  395000.00               1.6           136    1   \n",
       "\n",
       "     smoking  time  DEATH_EVENT  \n",
       "0          0     4            1  \n",
       "1          0     6            1  \n",
       "2          1     7            1  \n",
       "3          0     7            1  \n",
       "4          0     8            1  \n",
       "..       ...   ...          ...  \n",
       "294        1   270            0  \n",
       "295        0   271            0  \n",
       "296        0   278            0  \n",
       "297        1   280            0  \n",
       "298        1   285            0  \n",
       "\n",
       "[299 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598f101",
   "metadata": {},
   "source": [
    "# Cross validation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c41e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['time', \"DEATH_EVENT\"])\n",
    "y = data['time']   # pandas Series\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "# X.shape, y.shape print shapes of X and y to undestand their dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3341fec",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e127d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data based on training set\n",
    "\n",
    "def get_fold_data(X, y, train_idx, val_idx):\n",
    "   \n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_val   = X.iloc[val_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val   = y.iloc[val_idx]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def get_fold_data_normalized(X, y, train_idx, val_idx):\n",
    "   \n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_val   = X.iloc[val_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val   = y.iloc[val_idx]\n",
    "\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std  = X_train.std(axis=0)\n",
    "\n",
    "    y_train_mean = y_train.mean()\n",
    "\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm   = (X_val   - mean) / std\n",
    "    y_train = y_train - y_train_mean\n",
    "    y_val   = y_val   - y_train_mean\n",
    "\n",
    "    return X_train_norm, X_val_norm, y_train, y_val\n",
    "\n",
    "# Tensor conversion\n",
    "\n",
    "def torch_tensor_conversion(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor   = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_val_tensor   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    "\n",
    "def get_model(input_dim, hidden_dim, output_dim):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True),     # Input layer\n",
    "        torch.nn.Tanh(),                                                                # Activation function\n",
    "        torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True),    # Output layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7b1ee",
   "metadata": {},
   "source": [
    "## 2 layer cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105cc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "\n",
    "outer_folds_k_1 = 10\n",
    "inner_folds_k_2 = 10\n",
    "random_state = 42\n",
    "\n",
    "# ANN parameters\n",
    "input_dim  = M # M number of features\n",
    "output_dim = 1 # regression problem\n",
    "lr = 1e-3\n",
    "n_epochs = 10000\n",
    "momentum = 0.9\n",
    "hyperparameters_ANN = [1, 2, 3, 4, 5, 7, 10, 15, 20]  # hidden layer sizes to try\n",
    "\n",
    "\n",
    "# Regularization parameters for linear regression\n",
    "lambdas__for_linear_regression = np.logspace(-4, 3, 30)[23:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c6dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35.6224789   62.10169419 108.26367339 188.73918221 329.03445623]\n"
     ]
    }
   ],
   "source": [
    "print(lambdas__for_linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5b41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 - Inner Fold 1\n",
      "Outer Fold 1 - Inner Fold 2\n",
      "Outer Fold 1 - Inner Fold 3\n",
      "Outer Fold 1 - Inner Fold 4\n",
      "Outer Fold 1 - Inner Fold 5\n",
      "Outer Fold 1 - Inner Fold 6\n",
      "Outer Fold 1 - Inner Fold 7\n",
      "Outer Fold 1 - Inner Fold 8\n",
      "Outer Fold 1 - Inner Fold 9\n",
      "Outer Fold 1 - Inner Fold 10\n",
      "For outer fold 1 Best λ (alpha): 188.73918221350996, Test MSE: 5157.651809924351\n",
      "For outer fold 1 Best hidden units: 4, Test MSE: 6816.6270\n",
      "For outer fold 1 Mean Inner fold MSE for Baseline: 5543.878799813896\n",
      "Outer Fold 2 - Inner Fold 1\n",
      "Outer Fold 2 - Inner Fold 2\n",
      "Outer Fold 2 - Inner Fold 3\n",
      "Outer Fold 2 - Inner Fold 4\n",
      "Outer Fold 2 - Inner Fold 5\n",
      "Outer Fold 2 - Inner Fold 6\n",
      "Outer Fold 2 - Inner Fold 7\n",
      "Outer Fold 2 - Inner Fold 8\n",
      "Outer Fold 2 - Inner Fold 9\n",
      "Outer Fold 2 - Inner Fold 10\n",
      "For outer fold 2 Best λ (alpha): 108.2636733874054, Test MSE: 5539.99985068988\n",
      "For outer fold 2 Best hidden units: 3, Test MSE: 6061.3311\n",
      "For outer fold 2 Mean Inner fold MSE for Baseline: 5599.172962415298\n",
      "Outer Fold 3 - Inner Fold 1\n",
      "Outer Fold 3 - Inner Fold 2\n",
      "Outer Fold 3 - Inner Fold 3\n",
      "Outer Fold 3 - Inner Fold 4\n",
      "Outer Fold 3 - Inner Fold 5\n",
      "Outer Fold 3 - Inner Fold 6\n",
      "Outer Fold 3 - Inner Fold 7\n",
      "Outer Fold 3 - Inner Fold 8\n",
      "Outer Fold 3 - Inner Fold 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m outputs = model(X_train_inner_tensor)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_inner_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Make sure that the gradients are zero before you use backpropagation\u001b[39;00m\n\u001b[32m     68\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dtu02452/lib/python3.11/site-packages/torch/nn/functional.py:3339\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3336\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m   3338\u001b[39m expanded_input, expanded_target = torch.broadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[32m-> \u001b[39m\u001b[32m3339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "CV_outer = KFold(n_splits=outer_folds_k_1, shuffle=True, random_state=random_state) \n",
    "\n",
    "baseline_per_fold = {}   # Outer fold dict (key: outer fold index)\n",
    "best_hyperparameters_per_fold = {}\n",
    "best_lambda_per_fold = {}\n",
    "fold_results = {}\n",
    "outer_fold_index = 0\n",
    "\n",
    "for outer_train_idx, outer_test_idx in CV_outer.split(X):\n",
    "    outer_fold_index += 1\n",
    "    X_train_outer, X_test_outer, y_train_outer, y_test_outer = get_fold_data(X, y, outer_train_idx, outer_test_idx)\n",
    "\n",
    "    CV_inner = KFold(n_splits=inner_folds_k_2, shuffle=True, random_state=random_state)\n",
    "    inner_mse_ANN = {}\n",
    "    inner_mse_linear_regression = {}\n",
    "    inner_fold_index = 0\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "        inner_fold_index += 1\n",
    "        print(f\"Outer Fold {outer_fold_index} - Inner Fold {inner_fold_index}\")\n",
    "\n",
    "        ############################# DATA Inner Fold ####################################\n",
    "        X_train_inner_norm, X_test_inner_norm, y_train_inner_norm, y_test_inner_norm = get_fold_data_normalized(X_train_outer, y_train_outer, inner_train_idx, inner_test_idx)\n",
    "\n",
    "        ############################# Linear Regression Inner Fold ####################################\n",
    "        \n",
    "        # Set up a dictionary to store the results for each lambda setting\n",
    "        results_inner_linear_regression = {lam: {'train': [], 'test': []} for lam in lambdas__for_linear_regression}\n",
    "\n",
    "        for lam in lambdas__for_linear_regression:\n",
    "\n",
    "            model = Ridge(alpha=lam, random_state=42)\n",
    "            model.fit(X_train_inner_norm, y_train_inner_norm)\n",
    "\n",
    "            y_test_pred_inner = model.predict(X_test_inner_norm)\n",
    "            mse_test = mean_squared_error(y_test_inner_norm, y_test_pred_inner)\n",
    "\n",
    "            results_inner_linear_regression[lam]['test'].append(mse_test)\n",
    "            inner_mse_linear_regression[inner_fold_index] = results_inner_linear_regression\n",
    "\n",
    "        ############################# ANN Inner Fold ########################################\n",
    "        X_train_inner_tensor, y_train_inner_tensor, X_test_inner_tensor, y_test_inner_tensor = torch_tensor_conversion(X_train_inner_norm, y_train_inner_norm, X_test_inner_norm, y_test_inner_norm) \n",
    "    \n",
    "        # Set up a dictionary to store the results for each hyperparameter setting\n",
    "        results_inner_ANN = {hidden_dim: {'train': [], 'test': []} for hidden_dim in hyperparameters_ANN}\n",
    "\n",
    "        for hidden_dim in hyperparameters_ANN:\n",
    "            # Define a model instance with a specific number of hidden units\n",
    "            model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "            # Define loss criterion\n",
    "            criterion = torch.nn.MSELoss()\n",
    "\n",
    "            # Define the optimizer as the Adam optimizer (not needed to know the details)\n",
    "            optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "\n",
    "                # Set the model to training mode\n",
    "                model.train()\n",
    "\n",
    "                # Make a forward pass through the model to compute the outputs\n",
    "                outputs = model(X_train_inner_tensor)\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, y_train_inner_tensor)\n",
    "\n",
    "                # Make sure that the gradients are zero before you use backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                # Do a backward pass to compute the gradients wrt. model parameters using backpropagation.\n",
    "                loss.backward()\n",
    "                # Update the model parameters by making the optimizer take a gradient descent step\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Store the training loss for this epoch in the dictionary\n",
    "                #results_inner_ANN[hidden_dim]['train'].append(loss.item())\n",
    "\n",
    "            # Compute the final test loss on the test set\n",
    "            with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "                model.eval()\n",
    "                val_outputs = model(X_test_inner_tensor)\n",
    "                val_loss = criterion(val_outputs, y_test_inner_tensor)\n",
    "                results_inner_ANN[hidden_dim]['test'].append(val_loss.item())\n",
    "                #print(f'  Hidden units: {hidden_dim}, Validation set MSE: {val_loss.item():.4f}')\n",
    "                inner_mse_ANN[inner_fold_index] = results_inner_ANN \n",
    "\n",
    "        ############################# BASELINE Inner Fold ####################################\n",
    "\n",
    "        #----\n",
    "\n",
    "    ############################ OUTER FOLD ##########################################################\n",
    "\n",
    "    ############################ Data ##########################################################\n",
    "\n",
    "    X_train_outer_norm, X_test_outer_norm, y_train_outer_norm, y_test_outer_norm = get_fold_data_normalized(X, y, outer_train_idx, outer_test_idx)\n",
    "\n",
    "    X_train_outer_tensor, y_train_outer_tensor, X_test_outer_tensor, y_test_outer_tensor = torch_tensor_conversion(X_train_outer_norm, y_train_outer_norm, X_test_outer_norm, y_test_outer_norm)\n",
    "\n",
    "    ############################ Linear Regression Outer Fold ####################################\n",
    "\n",
    "    avg_mse_per_lambda = {}\n",
    "    for lam in lambdas__for_linear_regression:\n",
    "        mse_values = []\n",
    "        for inner_fold in inner_mse_linear_regression.keys():\n",
    "            mse_values.append(inner_mse_linear_regression[inner_fold][lam]['test'][0])  # We only have one value of test per fold \n",
    "        avg_mse = np.mean(mse_values)\n",
    "        avg_mse_per_lambda[lam] = avg_mse\n",
    "    \n",
    "    best_lambda = min(avg_mse_per_lambda, key=avg_mse_per_lambda.get)\n",
    "    best_lambda_per_fold[outer_fold_index] = best_lambda\n",
    "\n",
    "    model = Ridge(alpha=best_lambda, random_state=42)\n",
    "    model.fit(X_train_outer_norm, y_train_outer_norm)\n",
    "    y_test_pred_outer = model.predict(X_test_outer_norm)\n",
    "    mse_test_outer = mean_squared_error(y_test_outer_norm, y_test_pred_outer)\n",
    "    print(f\"For outer fold {outer_fold_index} Best λ (alpha): {best_lambda}, Test MSE: {mse_test_outer}\")\n",
    "\n",
    "    ############################ ANN Outer Fold ####################################\n",
    "    # Find the best hyperparameter based on inner folds\n",
    "    avg_mse_per_hyperparam = {}\n",
    "    for hidden_dim in hyperparameters_ANN:\n",
    "        mse_values = []\n",
    "        for inner_fold in inner_mse_ANN.keys():\n",
    "            mse_values.append(inner_mse_ANN[inner_fold][hidden_dim]['test'][0])  # We only have one value of test per fold \n",
    "        avg_mse = np.mean(mse_values)\n",
    "        avg_mse_per_hyperparam[hidden_dim] = avg_mse\n",
    "        \n",
    "    \n",
    "    best_hyperparam = min(avg_mse_per_hyperparam, key=avg_mse_per_hyperparam.get)\n",
    "    best_hyperparameters_per_fold[outer_fold_index] = best_hyperparam\n",
    "\n",
    "\n",
    "    model = get_model(input_dim=input_dim, hidden_dim=best_hyperparam, output_dim=output_dim)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        outputs = model(X_train_outer_tensor)\n",
    "        loss = criterion(outputs, y_train_outer_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "            model.eval()\n",
    "            val_outputs = model(X_test_outer_tensor)\n",
    "            val_loss = criterion(val_outputs, y_test_outer_tensor)\n",
    "            print(f'For outer fold {outer_fold_index} Best hidden units: {best_hyperparam}, Test MSE: {val_loss.item():.4f}')\n",
    "    \n",
    "    ############################ BASELINE Outer Fold ###############################\n",
    "\n",
    "    y_train_mean = y_train_outer_norm.mean()\n",
    "    y_test_pred_outer = pd.Series(y_train_mean, index=y_test_outer_norm.index)\n",
    "    outer_mse_baseline = mean_squared_error(y_test_outer_norm, y_test_pred_outer)\n",
    "    baseline_per_fold[outer_fold_index] = outer_mse_baseline \n",
    "    print(f\"For outer fold {outer_fold_index} Mean Inner fold MSE for Baseline:\", outer_mse_baseline)\n",
    "\n",
    "    ############################# STORE RESULTS ####################################\n",
    "\n",
    "    fold_results[outer_fold_index] = {\n",
    "        \"linear_regression_best_lambda\": best_lambda,\n",
    "        \"linear_regression_mse\": mse_test_outer,\n",
    "        \"ANN_best_hidden_units\": best_hyperparam,\n",
    "        \"ANN_mse\": val_loss.item(),\n",
    "        \"baseline_outer_mse\": outer_mse_baseline\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d4dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary of Outer Fold Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linear_regression_best_lambda</th>\n",
       "      <th>linear_regression_mse</th>\n",
       "      <th>ANN_best_hidden_units</th>\n",
       "      <th>ANN_mse</th>\n",
       "      <th>baseline_outer_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>4540.016476</td>\n",
       "      <td>2</td>\n",
       "      <td>5490.012207</td>\n",
       "      <td>5543.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>3954.948324</td>\n",
       "      <td>2</td>\n",
       "      <td>4194.473145</td>\n",
       "      <td>5599.172962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>4398.103458</td>\n",
       "      <td>4</td>\n",
       "      <td>4697.844727</td>\n",
       "      <td>6384.788366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>2426.851130</td>\n",
       "      <td>2</td>\n",
       "      <td>3658.928223</td>\n",
       "      <td>5103.847464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>3549.336027</td>\n",
       "      <td>3</td>\n",
       "      <td>3330.190674</td>\n",
       "      <td>4707.124051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>4027.736339</td>\n",
       "      <td>4</td>\n",
       "      <td>3220.433350</td>\n",
       "      <td>6811.592484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>4554.249669</td>\n",
       "      <td>3</td>\n",
       "      <td>4775.502441</td>\n",
       "      <td>6991.245497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>6434.358522</td>\n",
       "      <td>5</td>\n",
       "      <td>7355.668945</td>\n",
       "      <td>7346.281996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>5455.898533</td>\n",
       "      <td>3</td>\n",
       "      <td>5033.069336</td>\n",
       "      <td>5970.658638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35.622479</td>\n",
       "      <td>4446.963313</td>\n",
       "      <td>3</td>\n",
       "      <td>4465.690430</td>\n",
       "      <td>5815.648144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    linear_regression_best_lambda  linear_regression_mse  \\\n",
       "1                       35.622479            4540.016476   \n",
       "2                       35.622479            3954.948324   \n",
       "3                       35.622479            4398.103458   \n",
       "4                       35.622479            2426.851130   \n",
       "5                       35.622479            3549.336027   \n",
       "6                       35.622479            4027.736339   \n",
       "7                       35.622479            4554.249669   \n",
       "8                       35.622479            6434.358522   \n",
       "9                       35.622479            5455.898533   \n",
       "10                      35.622479            4446.963313   \n",
       "\n",
       "    ANN_best_hidden_units      ANN_mse  baseline_outer_mse  \n",
       "1                       2  5490.012207         5543.878800  \n",
       "2                       2  4194.473145         5599.172962  \n",
       "3                       4  4697.844727         6384.788366  \n",
       "4                       2  3658.928223         5103.847464  \n",
       "5                       3  3330.190674         4707.124051  \n",
       "6                       4  3220.433350         6811.592484  \n",
       "7                       3  4775.502441         6991.245497  \n",
       "8                       5  7355.668945         7346.281996  \n",
       "9                       3  5033.069336         5970.658638  \n",
       "10                      3  4465.690430         5815.648144  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_results_df = pd.DataFrame.from_dict(fold_results, orient='index')\n",
    "print(\"\\n=== Summary of Outer Fold Results ===\")\n",
    "outer_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39538fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_best_parameters(dict_parameters_for_each_outer_fold):\n",
    "    count_parameter_dict = {}\n",
    "    for outer_fold_index in dict_parameters_for_each_outer_fold.keys():\n",
    "        if dict_parameters_for_each_outer_fold.get(outer_fold_index) not in count_parameter_dict.keys() :\n",
    "            count_parameter_dict[dict_parameters_for_each_outer_fold.get(outer_fold_index)] = 1\n",
    "\n",
    "        else:\n",
    "            count_parameter_dict[dict_parameters_for_each_outer_fold.get(outer_fold_index)] += 1\n",
    "\n",
    "    return count_parameter_dict\n",
    "\n",
    "def best_parameter(count_parameter_dict):\n",
    "    best_param = max(count_parameter_dict, key=count_parameter_dict.get)\n",
    "    return best_param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ (alpha) for Linear Regression across all outer folds: 35.622478902624444\n",
      "Best hidden units for ANN across all outer folds: 3\n"
     ]
    }
   ],
   "source": [
    "best_lambda = best_parameter(count_best_parameters(best_lambda_per_fold))\n",
    "best_hyperparameter = best_parameter(count_best_parameters(best_hyperparameters_per_fold))\n",
    "    \n",
    "print(\"Best λ (alpha) for Linear Regression across all outer folds:\", best_lambda)\n",
    "print(\"Best hidden units for ANN across all outer folds:\", best_hyperparam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b20091",
   "metadata": {},
   "source": [
    "# Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ab9e3",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2aa45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_ttest(r, rho, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a correlated t-test to compare two models under Setup II.\n",
    "\n",
    "    Parameters:\n",
    "    - r (array-like): Array of performance differences across folds (e.g. r_j = error_A - error_B)\n",
    "    - rho (float): Correlation coefficient between folds (typically 1/K for K-fold CV)\n",
    "    - alpha (float, optional): Significance level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "    - p (float): p-value of the test\n",
    "    - CI (tuple): Confidence interval for the mean difference\n",
    "    \"\"\"\n",
    "\n",
    "    r = np.array(r)\n",
    "    r_hat = np.mean(r)\n",
    "    s_hat = np.std(r, ddof=1)\n",
    "    J = len(r)\n",
    "\n",
    "    # Adjusted standard deviation accounting for correlation\n",
    "    sigma_tilde = s_hat * np.sqrt((1 / J) + (rho / (1 - rho)))\n",
    "\n",
    "    # Confidence interval\n",
    "    CI = st.t.interval(1 - alpha, df=J - 1, loc=r_hat, scale=sigma_tilde)\n",
    "\n",
    "    # Two-sided p-value\n",
    "    p = 2 * st.t.cdf(-np.abs(r_hat) / sigma_tilde, df=J - 1)\n",
    "\n",
    "    return r_hat, CI, p\n",
    "\n",
    "def get_fold_data_normalized(X, y, train_idx, val_idx):\n",
    "   \n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_val   = X.iloc[val_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val   = y.iloc[val_idx]\n",
    "\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std  = X_train.std(axis=0)\n",
    "\n",
    "    y_train_mean = y_train.mean()\n",
    "\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm   = (X_val   - mean) / std\n",
    "    y_train = y_train - y_train_mean\n",
    "    y_val   = y_val   - y_train_mean\n",
    "\n",
    "    return X_train_norm, X_val_norm, y_train, y_val\n",
    "\n",
    "def torch_tensor_conversion(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor   = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_val_tensor   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf06ee4",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10 # Repetitions\n",
    "K = 10 # Folds\n",
    "rho = 1 / K # Correlation heuristic\n",
    "alpha = 0.05 # Significance level\n",
    "\n",
    "# ANN parameters\n",
    "\n",
    "input_dim  = M # M number of features\n",
    "output_dim = 1 # regression problem\n",
    "lr = 1e-3\n",
    "n_epochs = 1000\n",
    "momentum = 0.9\n",
    "\n",
    "#Loss Function \n",
    "l2_loss = lambda y, y_pred: (y - y_pred)**2\n",
    "loss_func = l2_loss # Loss function\n",
    "\n",
    "# Parameters used\n",
    "\n",
    "best_lambda_statistic_test = best_lambda\n",
    "best_hyperparameter_statistic_test = best_hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d4075",
   "metadata": {},
   "source": [
    "### ANN vs Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc8142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:02<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup II results:\n",
      "r_hat: -2258.0687\n",
      "95% CI: [-3237.4661, -1278.6713]\n",
      "p-value: 1.3836309516367945e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "\n",
    "for repeat_idx in range(m):\n",
    "    print(f\"Repetition {repeat_idx+1}/{m}\")\n",
    "\n",
    "    # 5.2) Initialize KFold cross-validation, set the seed to repeat_idx\n",
    "    ### BEGIN SOLUTION\n",
    "    CV_kfold = KFold(n_splits=K, shuffle=True, random_state=repeat_idx)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    for fold, (train_index, test_index) in tqdm(enumerate(CV_kfold.split(X)), total=CV_kfold.get_n_splits(X),desc=\"Cross-validation fold\"):\n",
    "        # Split data into training and test sets\n",
    "\n",
    "        ############################################# DATA #################################################\n",
    "\n",
    "        X_train_norm, X_test_norm, y_train_norm, y_test_norm= get_fold_data_normalized(X, y, train_index, test_index)\n",
    "\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = torch_tensor_conversion(X_train_norm, y_train_norm, X_test_norm, y_test_norm)\n",
    "\n",
    "        ############################################# LINEAR REGRESSION #################################################\n",
    "\n",
    "        model = Ridge(alpha = best_lambda_statistic_test, random_state=repeat_idx)\n",
    "        model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "        y_test_linear_reg = model.predict(X_test_norm)\n",
    "        loss_func_linear_reg = loss_func(y_test_norm, y_test_linear_reg).values.flatten()  # Get individual squared errors as a 1D array\n",
    "\n",
    "        ##################################################### ANN MODEL #################################################\n",
    "\n",
    "        model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "        # Define loss criterion - set reduction to 'none' to get individual errors\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        # Define the optimizer as the Adam optimizer (not needed to know the details)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # Make a forward pass through the model to compute the outputs\n",
    "            outputs = model(X_train_tensor)\n",
    "            # Compute the loss (this will still be a tensor of individual losses, so take mean for backward)\n",
    "            loss = criterion(outputs, y_train_tensor).mean()  # mean needed for backward\n",
    "\n",
    "            # Make sure that the gradients are zero before you use backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            # Do a backward pass to compute the gradients wrt. model parameters using backpropagation.\n",
    "            loss.backward()\n",
    "            # Update the model parameters by making the optimizer take a gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_outputs = model(X_test_tensor)\n",
    "            val_losses = criterion(val_outputs, y_test_tensor)  # Tensor of individual squared errors\n",
    "            #loss_func_ANN = val_losses.detach().cpu().numpy().flatten()  # Convert to numpy array for all individual errors\n",
    "            loss_func_ANN = np.array(val_losses.detach().cpu().view(-1).tolist())   \n",
    "        ######################################################### MODELS COMPARISON #######################################\n",
    "\n",
    "        r_j = np.mean(loss_func_linear_reg - loss_func_ANN)\n",
    "\n",
    "        r.append(r_j)\n",
    "\n",
    "# Calculate p-value and confidence interval using correlated t-test\n",
    "r_hat, CI, p_value = correlated_ttest(r, rho, alpha=alpha)\n",
    "\n",
    "print(f\"\\nSetup II results:\")\n",
    "print(f\"r_hat: {r_hat:.4f}\")\n",
    "print(f\"95% CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a30d7",
   "metadata": {},
   "source": [
    "### ANN vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdef24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_funcion_ANN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m             loss_func_ANN = np.array(val_losses.detach().cpu().view(-\u001b[32m1\u001b[39m).tolist())   \n\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m######################################################### MODELS COMPARISON #######################################\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         r_j = np.mean(loss_funcion_baseline - \u001b[43mloss_funcion_ANN\u001b[49m)\n\u001b[32m     62\u001b[39m         r.append(r_j)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Calculate p-value and confidence interval using correlated t-test\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'loss_funcion_ANN' is not defined"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "\n",
    "for repeat_idx in range(m):\n",
    "    print(f\"Repetition {repeat_idx+1}/{m}\")\n",
    "\n",
    "    # 5.2) Initialize KFold cross-validation, set the seed to repeat_idx\n",
    "    ### BEGIN SOLUTION\n",
    "    CV_kfold = KFold(n_splits=K, shuffle=True, random_state=repeat_idx)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    for fold, (train_index, test_index) in tqdm(enumerate(CV_kfold.split(X)), total=CV_kfold.get_n_splits(X),desc=\"Cross-validation fold\"):\n",
    "        # Split data into training and test sets\n",
    "\n",
    "        ############################################# DATA #################################################\n",
    "\n",
    "        X_train_norm, X_test_norm, y_train_norm, y_test_norm= get_fold_data_normalized(X, y, train_index, test_index)\n",
    "\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = torch_tensor_conversion(X_train_norm, y_train_norm, X_test_norm, y_test_norm)\n",
    "\n",
    "        ############################################# BASELINE #################################################\n",
    "\n",
    "        y_train_mean_baseline = y_train_norm.mean()\n",
    "        y_test_pred_baseline = pd.Series(y_train_mean_baseline, index=y_test_norm.index)\n",
    "        loss_funcion_baseline = loss_func(y_test_norm, y_test_pred_baseline).values.flatten()\n",
    "\n",
    "        ##################################################### ANN MODEL #################################################\n",
    "\n",
    "        model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "        # Define loss criterion - set reduction to 'none' to get individual errors\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        # Define the optimizer as the Adam optimizer (not needed to know the details)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # Make a forward pass through the model to compute the outputs\n",
    "            outputs = model(X_train_tensor)\n",
    "            # Compute the loss (this will still be a tensor of individual losses, so take mean for backward)\n",
    "            loss = criterion(outputs, y_train_tensor).mean()  # mean needed for backward\n",
    "\n",
    "            # Make sure that the gradients are zero before you use backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            # Do a backward pass to compute the gradients wrt. model parameters using backpropagation.\n",
    "            loss.backward()\n",
    "            # Update the model parameters by making the optimizer take a gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_outputs = model(X_test_tensor)\n",
    "            val_losses = criterion(val_outputs, y_test_tensor)  # Tensor of individual squared errors\n",
    "            #loss_func_ANN = val_losses.detach().cpu().numpy().flatten()  # Convert to numpy array for all individual errors\n",
    "            loss_func_ANN = np.array(val_losses.detach().cpu().view(-1).tolist())   \n",
    "        ######################################################### MODELS COMPARISON #######################################\n",
    "\n",
    "        r_j = np.mean(loss_funcion_baseline - loss_funcion_ANN)\n",
    "        r.append(r_j)\n",
    "\n",
    "# Calculate p-value and confidence interval using correlated t-test\n",
    "r_hat, CI, p_value = correlated_ttest(r, rho, alpha=alpha)\n",
    "\n",
    "print(f\"\\nSetup II results:\")\n",
    "print(f\"r_hat: {r_hat:.4f}\")\n",
    "print(f\"95% CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f77d6",
   "metadata": {},
   "source": [
    "### Linear Reg vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bcdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 87.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 162.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 118.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 261.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 210.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 294.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 278.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 402.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 387.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 100%|██████████| 10/10 [00:00<00:00, 287.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup II results:\n",
      "r_hat: -1612.9596\n",
      "95% CI: [-2210.0660, -1015.8533]\n",
      "p-value: 5.443246472963441e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "\n",
    "for repeat_idx in range(m):\n",
    "    print(f\"Repetition {repeat_idx+1}/{m}\")\n",
    "\n",
    "    # 5.2) Initialize KFold cross-validation, set the seed to repeat_idx\n",
    "    ### BEGIN SOLUTION\n",
    "    CV_kfold = KFold(n_splits=K, shuffle=True, random_state=repeat_idx)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    for fold, (train_index, test_index) in tqdm(enumerate(CV_kfold.split(X)), total=CV_kfold.get_n_splits(X),desc=\"Cross-validation fold\"):\n",
    "        # Split data into training and test sets\n",
    "\n",
    "        ############################################# DATA #################################################\n",
    "\n",
    "        X_train_norm, X_test_norm, y_train_norm, y_test_norm= get_fold_data_normalized(X, y, train_index, test_index)\n",
    "\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = torch_tensor_conversion(X_train_norm, y_train_norm, X_test_norm, y_test_norm)\n",
    "\n",
    "        ############################################# LINEAR REGRESSION #################################################\n",
    "\n",
    "        model = Ridge(alpha = best_lambda_statistic_test, random_state=repeat_idx)\n",
    "        model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "        y_test_linear_reg = model.predict(X_test_norm)\n",
    "        loss_func_linear_reg = loss_func(y_test_norm, y_test_linear_reg)\n",
    "\n",
    "        ############################################# BASELINE #################################################\n",
    "\n",
    "        y_train_mean_baseline = y_train_norm.mean()\n",
    "        y_test_pred_baseline = pd.Series(y_train_mean_baseline, index=y_test_norm.index)\n",
    "        loss_funcion_baseline = loss_func(y_test_norm, y_test_pred_baseline)\n",
    "\n",
    "\n",
    "        ######################################################### MODELS COMPARISON #######################################\n",
    "        \n",
    "        r_j = np.mean(loss_func_linear_reg - loss_funcion_baseline)\n",
    "\n",
    "        r.append(r_j)\n",
    "\n",
    "# Calculate p-value and confidence interval using correlated t-test\n",
    "r_hat, CI, p_value = correlated_ttest(r, rho, alpha=alpha)\n",
    "\n",
    "print(f\"\\nSetup II results:\")\n",
    "print(f\"r_hat: {r_hat:.4f}\")\n",
    "print(f\"95% CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c0fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
