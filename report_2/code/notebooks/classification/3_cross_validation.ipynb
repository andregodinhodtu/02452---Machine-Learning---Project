{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8caeade0",
   "metadata": {},
   "source": [
    "# 3_cross_validation_on_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04ba25",
   "metadata": {},
   "source": [
    "## Data import and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c13ac8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, zero_one_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d4a57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data_file=\"heart_failure_clinical_records_dataset\"\n",
    "\n",
    "data = pd.read_csv(f\"../../raw_data/{name_data_file}.csv\", na_values=[\"?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9d49120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 299 samples, 12 features\n"
     ]
    }
   ],
   "source": [
    "# Split the data frame into features and labels\n",
    "X = data.drop(columns=[\"DEATH_EVENT\"])\n",
    "y = data[\"DEATH_EVENT\"]\n",
    "\n",
    "N, M = X.shape\n",
    "print(f\"Data loaded: {N} samples, {M} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b0a53",
   "metadata": {},
   "source": [
    "# Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0eb3b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_data(X, y, train_idx, val_idx):\n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_val   = X.iloc[val_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val   = y.iloc[val_idx]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "def get_fold_data_normalized(X, y, train_idx, val_idx):\n",
    "    X_train, X_val, y_train, y_val = get_fold_data(X, y, train_idx, val_idx)\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std  = X_train.std(axis=0)\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm   = (X_val   - mean) / std\n",
    "    return X_train_norm, X_val_norm, y_train, y_val\n",
    "\n",
    "\n",
    "def torch_tensor_conversion(X_train, y_train, X_val, y_val):\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor   = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_val_tensor   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32).view(-1, 1)\n",
    "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeed493",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8a391ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_folds_k_1 = 10\n",
    "inner_folds_k_2 = 10\n",
    "random_state = 42\n",
    "\n",
    "# Hyperparameters to test\n",
    "hyperparameters_tree = [2, 3, 4, 5, 6, 7]       # e.g. max_depth values\n",
    "lambdas_logistic = np.logspace(-4, 3, 7) # e.g. inverse of regularization strength C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8257e2",
   "metadata": {},
   "source": [
    "# Cross_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8aa11d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OUTER FOLD 1\n",
      "Outer Fold 1 - Inner Fold 1\n",
      "Outer Fold 1 - Inner Fold 2\n",
      "Outer Fold 1 - Inner Fold 3\n",
      "Outer Fold 1 - Inner Fold 4\n",
      "Outer Fold 1 - Inner Fold 5\n",
      "Outer Fold 1 - Inner Fold 6\n",
      "Outer Fold 1 - Inner Fold 7\n",
      "Outer Fold 1 - Inner Fold 8\n",
      "Outer Fold 1 - Inner Fold 9\n",
      "Outer Fold 1 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.7667 | Error: 0.2333\n",
      "  ▶ Best Tree Depth: 4\n",
      "Finished Outer Fold 1\n",
      "  OUTER FOLD 2\n",
      "Outer Fold 2 - Inner Fold 1\n",
      "Outer Fold 2 - Inner Fold 2\n",
      "Outer Fold 2 - Inner Fold 3\n",
      "Outer Fold 2 - Inner Fold 4\n",
      "Outer Fold 2 - Inner Fold 5\n",
      "Outer Fold 2 - Inner Fold 6\n",
      "Outer Fold 2 - Inner Fold 7\n",
      "Outer Fold 2 - Inner Fold 8\n",
      "Outer Fold 2 - Inner Fold 9\n",
      "Outer Fold 2 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.8000 | Error: 0.2000\n",
      "  ▶ Best Tree Depth: 2\n",
      "Finished Outer Fold 2\n",
      "  OUTER FOLD 3\n",
      "Outer Fold 3 - Inner Fold 1\n",
      "Outer Fold 3 - Inner Fold 2\n",
      "Outer Fold 3 - Inner Fold 3\n",
      "Outer Fold 3 - Inner Fold 4\n",
      "Outer Fold 3 - Inner Fold 5\n",
      "Outer Fold 3 - Inner Fold 6\n",
      "Outer Fold 3 - Inner Fold 7\n",
      "Outer Fold 3 - Inner Fold 8\n",
      "Outer Fold 3 - Inner Fold 9\n",
      "Outer Fold 3 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.8333 | Error: 0.1667\n",
      "  ▶ Best Tree Depth: 2\n",
      "Finished Outer Fold 3\n",
      "  OUTER FOLD 4\n",
      "Outer Fold 4 - Inner Fold 1\n",
      "Outer Fold 4 - Inner Fold 2\n",
      "Outer Fold 4 - Inner Fold 3\n",
      "Outer Fold 4 - Inner Fold 4\n",
      "Outer Fold 4 - Inner Fold 5\n",
      "Outer Fold 4 - Inner Fold 6\n",
      "Outer Fold 4 - Inner Fold 7\n",
      "Outer Fold 4 - Inner Fold 8\n",
      "Outer Fold 4 - Inner Fold 9\n",
      "Outer Fold 4 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.8667 | Error: 0.1333\n",
      "  ▶ Best Tree Depth: 4\n",
      "Finished Outer Fold 4\n",
      "  OUTER FOLD 5\n",
      "Outer Fold 5 - Inner Fold 1\n",
      "Outer Fold 5 - Inner Fold 2\n",
      "Outer Fold 5 - Inner Fold 3\n",
      "Outer Fold 5 - Inner Fold 4\n",
      "Outer Fold 5 - Inner Fold 5\n",
      "Outer Fold 5 - Inner Fold 6\n",
      "Outer Fold 5 - Inner Fold 7\n",
      "Outer Fold 5 - Inner Fold 8\n",
      "Outer Fold 5 - Inner Fold 9\n",
      "Outer Fold 5 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.31623\n",
      "  Logistic Regression Outer Test Accuracy: 0.8000 | Error: 0.2000\n",
      "  ▶ Best Tree Depth: 4\n",
      "Finished Outer Fold 5\n",
      "  OUTER FOLD 6\n",
      "Outer Fold 6 - Inner Fold 1\n",
      "Outer Fold 6 - Inner Fold 2\n",
      "Outer Fold 6 - Inner Fold 3\n",
      "Outer Fold 6 - Inner Fold 4\n",
      "Outer Fold 6 - Inner Fold 5\n",
      "Outer Fold 6 - Inner Fold 6\n",
      "Outer Fold 6 - Inner Fold 7\n",
      "Outer Fold 6 - Inner Fold 8\n",
      "Outer Fold 6 - Inner Fold 9\n",
      "Outer Fold 6 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.9667 | Error: 0.0333\n",
      "  ▶ Best Tree Depth: 2\n",
      "Finished Outer Fold 6\n",
      "  OUTER FOLD 7\n",
      "Outer Fold 7 - Inner Fold 1\n",
      "Outer Fold 7 - Inner Fold 2\n",
      "Outer Fold 7 - Inner Fold 3\n",
      "Outer Fold 7 - Inner Fold 4\n",
      "Outer Fold 7 - Inner Fold 5\n",
      "Outer Fold 7 - Inner Fold 6\n",
      "Outer Fold 7 - Inner Fold 7\n",
      "Outer Fold 7 - Inner Fold 8\n",
      "Outer Fold 7 - Inner Fold 9\n",
      "Outer Fold 7 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 4.64159\n",
      "  Logistic Regression Outer Test Accuracy: 0.8667 | Error: 0.1333\n",
      "  ▶ Best Tree Depth: 2\n",
      "Finished Outer Fold 7\n",
      "  OUTER FOLD 8\n",
      "Outer Fold 8 - Inner Fold 1\n",
      "Outer Fold 8 - Inner Fold 2\n",
      "Outer Fold 8 - Inner Fold 3\n",
      "Outer Fold 8 - Inner Fold 4\n",
      "Outer Fold 8 - Inner Fold 5\n",
      "Outer Fold 8 - Inner Fold 6\n",
      "Outer Fold 8 - Inner Fold 7\n",
      "Outer Fold 8 - Inner Fold 8\n",
      "Outer Fold 8 - Inner Fold 9\n",
      "Outer Fold 8 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 4.64159\n",
      "  Logistic Regression Outer Test Accuracy: 0.8000 | Error: 0.2000\n",
      "  ▶ Best Tree Depth: 5\n",
      "Finished Outer Fold 8\n",
      "  OUTER FOLD 9\n",
      "Outer Fold 9 - Inner Fold 1\n",
      "Outer Fold 9 - Inner Fold 2\n",
      "Outer Fold 9 - Inner Fold 3\n",
      "Outer Fold 9 - Inner Fold 4\n",
      "Outer Fold 9 - Inner Fold 5\n",
      "Outer Fold 9 - Inner Fold 6\n",
      "Outer Fold 9 - Inner Fold 7\n",
      "Outer Fold 9 - Inner Fold 8\n",
      "Outer Fold 9 - Inner Fold 9\n",
      "Outer Fold 9 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 4.64159\n",
      "  Logistic Regression Outer Test Accuracy: 0.6000 | Error: 0.4000\n",
      "  ▶ Best Tree Depth: 2\n",
      "Finished Outer Fold 9\n",
      "  OUTER FOLD 10\n",
      "Outer Fold 10 - Inner Fold 1\n",
      "Outer Fold 10 - Inner Fold 2\n",
      "Outer Fold 10 - Inner Fold 3\n",
      "Outer Fold 10 - Inner Fold 4\n",
      "Outer Fold 10 - Inner Fold 5\n",
      "Outer Fold 10 - Inner Fold 6\n",
      "Outer Fold 10 - Inner Fold 7\n",
      "Outer Fold 10 - Inner Fold 8\n",
      "Outer Fold 10 - Inner Fold 9\n",
      "Outer Fold 10 - Inner Fold 10\n",
      "  ▶ Best λ (regularization): 0.00010\n",
      "  Logistic Regression Outer Test Accuracy: 0.8621 | Error: 0.1379\n",
      "  ▶ Best Tree Depth: 3\n",
      "Finished Outer Fold 10\n",
      " Nested CV Completed \n",
      "Best Logistic Regression λ per fold:\n",
      "{1: np.float64(0.0001), 2: np.float64(0.0001), 3: np.float64(0.0001), 4: np.float64(0.0001), 5: np.float64(0.31622776601683794), 6: np.float64(0.0001), 7: np.float64(4.641588833612782), 8: np.float64(4.641588833612782), 9: np.float64(4.641588833612782), 10: np.float64(0.0001)}\n",
      "Best Tree Depth per fold:\n",
      "{1: 4, 2: 2, 3: 2, 4: 4, 5: 4, 6: 2, 7: 2, 8: 5, 9: 2, 10: 3}\n",
      "\n",
      "=== Summary of Outer Fold Results ===\n",
      "    baseline_mean_inner_acc  baseline_outer_acc  baseline_outer_error  \\\n",
      "1                  0.688034            0.600000              0.400000   \n",
      "2                  0.691168            0.566667              0.433333   \n",
      "3                  0.687607            0.600000              0.400000   \n",
      "4                  0.691311            0.566667              0.433333   \n",
      "5                  0.661966            0.833333              0.166667   \n",
      "6                  0.676781            0.700000              0.300000   \n",
      "7                  0.671937            0.733333              0.266667   \n",
      "8                  0.669231            0.766667              0.233333   \n",
      "9                  0.687749            0.600000              0.400000   \n",
      "10                 0.662963            0.827586              0.172414   \n",
      "\n",
      "    logistic_best_lambda  logistic_outer_acc  logistic_outer_error  \\\n",
      "1               0.000100            0.766667              0.233333   \n",
      "2               0.000100            0.800000              0.200000   \n",
      "3               0.000100            0.833333              0.166667   \n",
      "4               0.000100            0.866667              0.133333   \n",
      "5               0.316228            0.800000              0.200000   \n",
      "6               0.000100            0.966667              0.033333   \n",
      "7               4.641589            0.866667              0.133333   \n",
      "8               4.641589            0.800000              0.200000   \n",
      "9               4.641589            0.600000              0.400000   \n",
      "10              0.000100            0.862069              0.137931   \n",
      "\n",
      "    tree_best_depth  tree_outer_acc  tree_outer_error  \n",
      "1                 4        0.766667          0.233333  \n",
      "2                 2        0.666667          0.333333  \n",
      "3                 2        0.800000          0.200000  \n",
      "4                 4        0.833333          0.166667  \n",
      "5                 4        0.766667          0.233333  \n",
      "6                 2        0.966667          0.033333  \n",
      "7                 2        0.900000          0.100000  \n",
      "8                 5        0.766667          0.233333  \n",
      "9                 2        0.833333          0.166667  \n",
      "10                3        0.931034          0.068966  \n"
     ]
    }
   ],
   "source": [
    "# Outer CV\n",
    "CV_outer = KFold(n_splits=outer_folds_k_1, shuffle=True, random_state=random_state)\n",
    "\n",
    "fold_results = {}\n",
    "best_hyperparameters_logistic = {}\n",
    "best_hyperparameters_tree = {}\n",
    "outer_fold_index = 0\n",
    "\n",
    "for outer_train_idx, outer_test_idx in CV_outer.split(X, y):\n",
    "    outer_fold_index += 1\n",
    "    print(f\"  OUTER FOLD {outer_fold_index}\")\n",
    "\n",
    "    X_train_outer, X_test_outer = X.iloc[outer_train_idx].copy(), X.iloc[outer_test_idx].copy()\n",
    "    y_train_outer, y_test_outer = y.iloc[outer_train_idx].copy(), y.iloc[outer_test_idx].copy()\n",
    "\n",
    "    mu_X_train = X_train_outer.mean(axis=0)\n",
    "    sigma_X_train = X_train_outer.std(axis=0, ddof=0)\n",
    "    X_train_outer_scaled = (X_train_outer - mu_X_train) / sigma_X_train\n",
    "    X_test_outer_scaled  = (X_test_outer  - mu_X_train) / sigma_X_train\n",
    "\n",
    "    CV_inner = KFold(n_splits=inner_folds_k_2, shuffle=True, random_state=outer_fold_index)\n",
    "    inner_acc_baseline = {}\n",
    "    inner_acc_logistic = {}\n",
    "    inner_acc_tree = {}\n",
    "    inner_fold_index = 0\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer_scaled, y_train_outer):\n",
    "        inner_fold_index += 1\n",
    "        print(f\"Outer Fold {outer_fold_index} - Inner Fold {inner_fold_index}\")\n",
    "\n",
    "        X_train_inner = X_train_outer_scaled.iloc[inner_train_idx]\n",
    "        X_val_inner   = X_train_outer_scaled.iloc[inner_test_idx]\n",
    "        y_train_inner = y_train_outer.iloc[inner_train_idx]\n",
    "        y_val_inner   = y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "        ############################# BASELINE Inner Fold ####################################\n",
    "        majority_class = y_train_inner.mode()[0]\n",
    "        y_val_pred = np.full_like(y_val_inner, fill_value=majority_class)\n",
    "        acc = accuracy_score(y_val_inner, y_val_pred)\n",
    "        inner_acc_baseline[inner_fold_index] = acc\n",
    "\n",
    "        ############################# LOGISTIC REGRESSION Inner Fold ####################################\n",
    "        results_inner_logistic = {lam: {'val_error': []} for lam in lambdas_logistic}\n",
    "\n",
    "        for lam in lambdas_logistic:\n",
    "            model = LogisticRegression(penalty=\"l2\", C=1/lam, solver=\"lbfgs\",\n",
    "                                       max_iter=1000, random_state=random_state)\n",
    "            model.fit(X_train_inner, y_train_inner)\n",
    "            y_val_pred = model.predict(X_val_inner)\n",
    "            val_error = zero_one_loss(y_val_inner, y_val_pred)\n",
    "            results_inner_logistic[lam]['val_error'].append(val_error)\n",
    "\n",
    "        inner_acc_logistic[inner_fold_index] = results_inner_logistic\n",
    "\n",
    "        ############################# DECISION TREE Inner Fold ####################################\n",
    "        results_inner_tree = {depth: {'val_error': []} for depth in hyperparameters_tree}\n",
    "\n",
    "        for depth in hyperparameters_tree:\n",
    "            tree = DecisionTreeClassifier(max_depth=depth, criterion='log_loss', random_state=random_state)\n",
    "            tree.fit(X_train_inner, y_train_inner)\n",
    "            y_val_pred = tree.predict(X_val_inner)\n",
    "            val_error = zero_one_loss(y_val_inner, y_val_pred)\n",
    "            results_inner_tree[depth]['val_error'].append(val_error)\n",
    "\n",
    "        inner_acc_tree[inner_fold_index] = results_inner_tree\n",
    "\n",
    "    ############################# LOGISTIC REGRESSION Outer Fold ####################################\n",
    "    avg_val_error_per_lambda = {}\n",
    "    for lam in lambdas_logistic:\n",
    "        val_errors = []\n",
    "        for inner_fold in inner_acc_logistic.keys():\n",
    "            val_errors.extend(inner_acc_logistic[inner_fold][lam]['val_error'])\n",
    "        avg_val_error_per_lambda[lam] = np.mean(val_errors)\n",
    "\n",
    "    best_lambda = min(avg_val_error_per_lambda, key=avg_val_error_per_lambda.get)\n",
    "    best_hyperparameters_logistic[outer_fold_index] = best_lambda\n",
    "    print(f\"  ▶ Best λ (regularization): {best_lambda:.5f}\")\n",
    "\n",
    "    final_model = LogisticRegression(penalty=\"l2\", C=1/best_lambda, solver=\"lbfgs\",\n",
    "                                     max_iter=1000, random_state=random_state)\n",
    "    final_model.fit(X_train_outer_scaled, y_train_outer)\n",
    "    y_test_pred = final_model.predict(X_test_outer_scaled)\n",
    "    outer_test_error = zero_one_loss(y_test_outer, y_test_pred)\n",
    "    outer_test_acc = accuracy_score(y_test_outer, y_test_pred)\n",
    "\n",
    "    print(f\"  Logistic Regression Outer Test Accuracy: {outer_test_acc:.4f} | Error: {outer_test_error:.4f}\")\n",
    "\n",
    "    ############################# BASELINE Outer Fold ####################################\n",
    "    mean_acc_inner_baseline = np.mean(list(inner_acc_baseline.values()))\n",
    "    majority_class_outer = y_train_outer.mode()[0]\n",
    "    y_pred_baseline_outer = np.full_like(y_test_outer, fill_value=majority_class_outer)\n",
    "    baseline_acc_outer = accuracy_score(y_test_outer, y_pred_baseline_outer)\n",
    "    baseline_error_outer = zero_one_loss(y_test_outer, y_pred_baseline_outer)\n",
    "\n",
    "    ############################# DECISION TREE Outer Fold ####################################\n",
    "    avg_val_error_per_depth = {\n",
    "        depth: np.mean([err\n",
    "                        for fold in inner_acc_tree.keys()\n",
    "                        for err in inner_acc_tree[fold][depth]['val_error']])\n",
    "        for depth in hyperparameters_tree\n",
    "    }\n",
    "    best_depth = min(avg_val_error_per_depth, key=avg_val_error_per_depth.get)\n",
    "    best_hyperparameters_tree[outer_fold_index] = best_depth\n",
    "    print(f\"  ▶ Best Tree Depth: {best_depth}\")\n",
    "\n",
    "    final_tree = DecisionTreeClassifier(max_depth=best_depth, criterion='log_loss', random_state=random_state)\n",
    "    final_tree.fit(X_train_outer_scaled, y_train_outer)\n",
    "    y_test_pred_tree = final_tree.predict(X_test_outer_scaled)\n",
    "    outer_test_error_tree = zero_one_loss(y_test_outer, y_test_pred_tree)\n",
    "    outer_test_acc_tree = accuracy_score(y_test_outer, y_test_pred_tree)\n",
    "\n",
    "    ############################# STORE RESULTS ####################################\n",
    "    fold_results[outer_fold_index] = {\n",
    "        \"baseline_mean_inner_acc\": mean_acc_inner_baseline,\n",
    "        \"baseline_outer_acc\": baseline_acc_outer,\n",
    "        \"baseline_outer_error\": baseline_error_outer,\n",
    "        \"logistic_best_lambda\": best_lambda,\n",
    "        \"logistic_outer_acc\": outer_test_acc,\n",
    "        \"logistic_outer_error\": outer_test_error,\n",
    "        \"tree_best_depth\": best_depth,\n",
    "        \"tree_outer_acc\": outer_test_acc_tree,\n",
    "        \"tree_outer_error\": outer_test_error_tree\n",
    "    }\n",
    "\n",
    "    print(f\"Finished Outer Fold {outer_fold_index}\")\n",
    "\n",
    "print(\" Nested CV Completed \")\n",
    "print(\"Best Logistic Regression λ per fold:\")\n",
    "print(best_hyperparameters_logistic)\n",
    "print(\"Best Tree Depth per fold:\")\n",
    "print(best_hyperparameters_tree)\n",
    "\n",
    "outer_results_df = pd.DataFrame.from_dict(fold_results, orient='index')\n",
    "print(\"\\n=== Summary of Outer Fold Results ===\")\n",
    "print(outer_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ffbef",
   "metadata": {},
   "source": [
    "# Cross validation of the methods (baleine included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0a2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: For outer fold 1 Mean Inner fold MSE: 0.31168091168091167\n",
      "Baseline: For outer fold 2 Mean Inner fold MSE: 0.30826210826210826\n",
      "Baseline: For outer fold 3 Mean Inner fold MSE: 0.3121082621082621\n",
      "Baseline: For outer fold 4 Mean Inner fold MSE: 0.3085470085470085\n",
      "Baseline: For outer fold 5 Mean Inner fold MSE: 0.3384615384615385\n",
      "Baseline: For outer fold 6 Mean Inner fold MSE: 0.3232193732193732\n",
      "Baseline: For outer fold 7 Mean Inner fold MSE: 0.3264957264957265\n",
      "Baseline: For outer fold 8 Mean Inner fold MSE: 0.33048433048433046\n",
      "Baseline: For outer fold 9 Mean Inner fold MSE: 0.3121082621082621\n",
      "Baseline: For outer fold 10 Mean Inner fold MSE: 0.337037037037037\n"
     ]
    }
   ],
   "source": [
    "outer_folds_k_1 = 10\n",
    "inner_folds_k_2 = 10\n",
    "random_state = 42\n",
    "\n",
    "CV_outer = KFold(n_splits=outer_folds_k_1, shuffle=True, random_state=random_state) \n",
    "\n",
    "fold_results = []  # store per-fold errors\n",
    "outer_test_mse = []\n",
    "outer_fold_index = 0\n",
    "inner_fold_index = 0\n",
    "\n",
    "for outer_train_idx, outer_test_idx in CV_outer.split(X):\n",
    "    outer_fold_index += 1\n",
    "    # KFold object returns indices for train and test sets with shuffling meaning that indices are not sequential, use print to verify\n",
    "    # .split() returns the indices of the samples for each fold, test and train sets\n",
    "\n",
    "    X_train_outer, X_test_outer = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "    #print(X_train_outer.shape, X_test_outer.shape)\n",
    "    #print(outer_train_idx)\n",
    "    #print(outer_test_idx)\n",
    "    y_train_outer, y_test_outer = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "    #print(y_train_outer.shape, y_test_outer.shape)\n",
    "\n",
    "    # let´s start of with the inner cross validation\n",
    "\n",
    "    CV_inner = KFold(n_splits=inner_folds_k_2, shuffle=True, random_state=random_state) # folds for inner cross-validation object\n",
    "    inner_mse_baseline = [] # store per-fold errors for inner CV\n",
    "    for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer): \n",
    "        inner_fold_index += 1\n",
    "\n",
    "        X_train_inner, X_test_inner = X_train_outer.iloc[inner_train_idx], X_train_outer.iloc[inner_test_idx]\n",
    "        y_train_inner, y_test_inner = y_train_outer.iloc[inner_train_idx], y_train_outer.iloc[inner_test_idx] \n",
    "\n",
    "        y_train_largest_class = y_train_inner.mode()[0] # calculate mode of y in training set, values are 0 and 1, so mode is the largest class\n",
    "        #print(f\"y_train_largest_class: {y_train_largest_class}\")\n",
    "        #print(f\"y_train_mean: {y_train_mean}\")\n",
    "        y_test_pred_inner = pd.Series(y_train_largest_class, index=y_test_inner.index)\n",
    "        #print(y_test_pred_inner)\n",
    "        #print(\"\\n\")\n",
    "        #print(f\"Number of 0s in y_test_inner: {sum(y_test_inner == 0)}, Number of 1s in y_test_inner: {sum(y_test_inner == 1)}\")\n",
    "\n",
    "        inner_mse_baseline.append(mean_squared_error(y_test_inner, y_test_pred_inner)) # calculate MSE for this inner fold, makes mean \n",
    "        #print(inner_mse_baseline)\n",
    "        # value of the differences between predicted and actual y values for the test set\n",
    "\n",
    "    inner_mse_mean = np.mean(inner_mse_baseline) # average MSE across inner folds\n",
    "    print(f\"Baseline: For outer fold {outer_fold_index} Mean Inner fold MSE:\", inner_mse_mean) # just a print to see progress, in reality we will have to do the cross validation using 3 models\n",
    "    # and the one who has the lowest inner MSE will be selected for the outer test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ced5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
